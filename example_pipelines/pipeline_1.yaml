connection:
  server: INSERT_SERVERNAME_HERE
  sql_login: INSERT_LOGIN_HERE
  password: INSERT_PASSWORD_HERE


# Number of tables to sync in parallel (default: 1 = sequential).
parallelism: 4

# Single directory for this pipeline: data, watermarks, and output manifest are written under it.
storage:
  ingest_pipeline: INSERT_UC_VOLUME_PATH_HERE
  # output_format: per_table              # (default) native Parquet per table
  # output_format: unified                # Lakeflow-Connect-style bronze envelope schema
  # parquet_compression: zstd             # (default) or snappy, gzip, brotli, lz4, none
  # parquet_compression_level: 3          # optional; ZSTD 1-22
  # row_group_size: 500000                # (default) rows per Parquet row group; smaller = less memory

databases:
  database_1:
    uc_catalog: INSERT_CATALOG_NAME_HERE    # Databricks Unity Catalog name (not used by sync)
    schemas:
      dbo:
        uc_schema: INSERT_SCHEMA_NAME_HERE  # Databricks Unity Catalog schema (not used by sync)
        tables:
          table_1:
            mode: full_incremental
            scd_type: 1
            soft_delete: true                # keep deleted rows with _is_deleted flag
          table_59:
            mode: full_incremental
            scd_type: 2                      # SCD Type II: track historical changes
