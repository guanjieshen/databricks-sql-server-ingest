---
description: Guide for the ingestion_pipeline_examples directory — Databricks DLT pipeline that consumes azsql_ct unified bronze output
globs: ingestion_pipeline_examples/**/*.py
alwaysApply: false
---

# Ingestion Pipeline Examples

Databricks Lakeflow Declarative Pipelines (DLT) code that reads the Parquet files produced by `azsql_ct` (unified output format) and materializes bronze + silver Delta tables with auto CDC.

## Files

| File | Purpose |
|---|---|
| `ingestion_pipeline_materialized.py` | Main DLT pipeline: bronze materialized temp table → per-table silver streaming tables with `create_auto_cdc_flow` |
| `metadata_helper.py` | Reads pipeline YAML → manifest (`output.yaml` or `incremental_output.yaml`) → per-table `schema.json`; returns `(table_configs, data_path)` |

## Architecture: Two-Layer Medallion

```
Cloud Storage (Parquet from azsql_ct)
  └─ landing_raw  [Bronze — temporary materialized Delta table]
       └─ _view_{catalog}_{schema}_{table}  [per-table filter + from_json]
            └─ {catalog}.{schema}.{table}   [Silver — streaming Delta with auto CDC]
```

### Bronze: `landing_raw`
- `@dp.table(temporary=True)` — materialized as Delta but not published to Unity Catalog.
- Reads **all** Parquet files under `data_path` via Auto Loader (`cloudFiles`).
- Materializing once avoids re-reading cloud storage N times for N tables.

### Silver: per-table streaming tables
- Generated dynamically by looping over `table_configs` from `metadata_helper.parse_output_yaml()`.
- Each table gets a **view** that filters `landing_raw` by `table_id.uoid` and calls `from_json("data", spark_schema)`.
- Each table gets a **streaming table** with `create_auto_cdc_flow` for SCD Type 1 or 2.
- `_is_deleted` column is added when `soft_delete=True`; otherwise `apply_as_deletes` expression handles DELETE operations.

## Unified Bronze Envelope Schema (Parquet)

The Parquet files produced by `azsql_ct.writer.UnifiedParquetWriter` use this schema:

| Column | Type | Description |
|---|---|---|
| `data` | `string` | JSON-serialized row (all non-CT columns) |
| `table_id` | `struct{catalog, schema, name, uoid}` | Source table identifier; `uoid` is a deterministic UUID5 |
| `cursor` | `struct{lsn, seqNum, sequence, timestamp}` | Change tracking position; `seqNum` = `SYS_CHANGE_VERSION` |
| `extractionTimestamp` | `int64` | Epoch milliseconds when sync ran |
| `operation` | `string` | `INSERT`, `UPDATE`, `DELETE`, or `LOAD` (full sync) |
| `schemaVersion` | `int64` | Deterministic hash of source column names/types |

## Key Identifiers

- **UOID** (`table_id.uoid`): deterministic UUID5 from `(database, schema, table)`. Used to filter `landing_raw` for a specific source table. Generated by `azsql_ct.writer._make_uoid()` and mirrored in `metadata_helper._make_uoid()`.
- **`seqNum`** (`cursor.seqNum`): string of `SYS_CHANGE_VERSION`. Used as `sequence_by` in `create_auto_cdc_flow` to order changes.

## MSSQL-to-Spark Type Mapping

`MSSQL_TO_SPARK` dict in `ingestion_pipeline_materialized.py` maps SQL Server type names (from `schema.json`) to PySpark types. Used to build the `StructType` for `from_json` parsing of the `data` column. Falls back to `StringType()` for unknown types.

## Config Flow

```
pipeline YAML (spark conf `input_yaml`)
  → storage.ingest_pipeline path
    → {ingest_pipeline}/output.yaml or incremental_output.yaml  (which tables, UC names, PKs, SCD types)
    → {ingest_pipeline}/watermarks/{db}/{schema}/{table}/schema.json  (column definitions)
    → {ingest_pipeline}/data/          (Parquet files — Auto Loader source)
```

`parse_output_yaml()` walks the nested manifest structure (`databases → db → schema → table`) and joins each table with its `schema.json` columns. Returns a flat list of table config dicts with keys: `uc_catalog`, `uc_schema`, `uc_table`, `database`, `schema`, `table`, `uoid`, `primary_key`, `scd_type`, `soft_delete`, `columns`.

**Manifest selection**: Spark conf `manifest_file` (default `"output.yaml"`) can be set to `"incremental_output.yaml"` to process only tables with changes from the last sync. Falls back to `output.yaml` if incremental is missing or empty.

## Important: UOID Must Match

`metadata_helper._make_uoid()` and `azsql_ct.writer._make_uoid()` both use null-byte separator (`\x00`) so UOIDs match. If you modify either, update both to keep filtering by `table_id.uoid` working.
